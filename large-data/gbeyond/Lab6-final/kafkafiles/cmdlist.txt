# 启动服务器

# 登录服务器
ssh root@node1ip
ssh root@node2ip
ssh root@node3ip
ssh root@node4ip


# 上传文件
# SFTP 版
sftp root@node1ip
lcd Filepath
lpwd # 检查本机路径是否正确
put flink-1.8.0-bin-scala_2.11.tgz
put apache-zookeeper-3.5.7-bin.tar.gz
put flink-shaded-hadoop-2-uber-2.7.5-10.0.jar
put kafka_2.10-0.8.2.1.tgz
put WordCount.jar
put wordcount.txt
exit

# SCP 版
scp flink-1.8.0-bin-scala_2.11.tgz root@node1ip:~/
scp apache-zookeeper-3.5.7-bin.tar.gz root@node1ip:~/
scp flink-shaded-hadoop-2-uber-2.7.5-10.0.jar root@node1ip:~/
scp kafka_2.10-0.8.2.1.tgz root@node1ip:~/
scp WordCount.jar root@node1ip:~/
scp wordcount.txt root@node1ip:~/

# 文件移动与解压
cp flink-1.8.0-bin-scala_2.11.tgz /home/modules/
cp apache-zookeeper-3.5.7-bin.tar.gz /home/modules/
cp kafka_2.10-0.8.2.1.tgz /home/modules/
cd /home/modules/
tar -zxvf flink-1.8.0-bin-scala_2.11.tgz
tar -xzvf apache-zookeeper-3.5.7-bin.tar.gz
tar -xzvf kafka_2.10-0.8.2.1.tgz
cd ~
cp flink-shaded-hadoop-2-uber-2.7.5-10.0.jar /home/modules/flink-1.8.0/lib/

# 配置全局环境变量,四个节点
vim /etc/profile

## {/etc/profile: 添加以下内容
# Flink
export FLINK_HOME=/home/modules/flink-1.8.0
export PATH=$FLINK_HOME/bin:$PATH
# Zookeeper
export ZK_HOME=/home/modules/apache-zookeeper-3.5.7-bin
export PATH=$ZK_HOME/bin:$PATH
# Kafka
export KAFKA_HOME=/home/modules/kafka_2.10-0.8.2.1
export PATH=$KAFKA_HOME/bin:$PATH
## end}

source /etc/profile
# On node1 ~
cd /home/modules/flink-1.8.0/conf/
# 更改Flink配置文件
vim flink-conf.yaml

## {flink-conf.yaml:添加如下配置
# 指定taskmananger的地址，如果是单机部署，指定localhost
taskmanager.host: localhost
## end}

start-cluster.sh # 脚本启动Flink进程
jps #查看最新启动的两个进程
# TODO: 截图1.1, 只用主节点jps

访问Flink的Web管理界面: http://node1ip:8081/#/overview
sudo yum -y install nc # 安装nc工具
# 在node1上使用nc命令向Socket发送单词。
nc -lk 9000
# TODO: 截图1.2, 单词

# 另外打开一个node1的shell页面，在node1上启动Flink自带的单词统计程序，接收输入的Socket数据并进行统计。
cd /home/modules/flink-1.8.0
bin/flink run examples/streaming/SocketWindowWordCount.jar --hostname localhost --port 9000

# 查看统计结果
# 在Flink的Web管理界面进入Task Managers目录下，选择Stdout选项卡，得到统计结果。
# TODO: 截图1.2, Web页面
# Flink自带的测试用例统计结果在log文件夹路径下。在node1上执行以下命令查看统计结果。tail命令第三个参数是文件名，根据实际情况来
cd /home/modules/flink-1.8.0/log/
tail -200f flink-root-taskexecutor-0-node1.out
# TODO: 截图1.2, 终端窗口

# 关闭local模式。
stop-cluster.sh

**注释掉 node1 节点 hosts 文件中 127 开头部分**
cd /home/modules/flink-1.8.0/conf/
vim flink-conf.yaml # 修改Flink配置文件
vim slaves # 修改slaves配置文件。
## {flink-conf.yaml:删除实验1中进行的修改( taskmanager.host: localhost 那行); 找到包含 jobmanager.rpc.address 的那行，替换成下面的内容
# 指定JobManager所在的服务器为node1
jobmanager.rpc.address: node1
## end }

## {slaves:替换原内容
node1
node2
node3
node4
## end}

# 分发配置文件
cd /home/modules/
scp -r flink-1.8.0 root@node2:$PWD/
scp -r flink-1.8.0 root@node3:$PWD/
scp -r flink-1.8.0 root@node4:$PWD/

# 启动Flink集群
start-cluster.sh
jps
# TODO: 截图2.1, 4台服务器都运行jps, 共4张

进入Web管理页面能看到Task Managers和Task Slots数量为4，说明集群正确启动。
http://node1ip:8081/#/overview
# TODO: 截图2.1, Web页面

# 运行Flink自带测试用例
# 在node1启动Socket服务，输入单词。
nc -lk 9000
# TODO: 单词2.2

# 另外打开一个node1的shell页面，在node1上启动Flink自带的单词统计程序，接收输入的Socket数据并进行统计。
cd /home/modules/flink-1.8.0
bin/flink run examples/streaming/SocketWindowWordCount.jar --hostname node1 --port 9000

# 在Web管理界面的Task Managers目录中，选择Free Slots为 0 的一项，选中后可以在它对应的Stdout中看到单词的统计结果。
# TODO: 截图2.2, Web页面

# 根据执行的Task Manager，在Free Slots为 0 的服务器的命令行中执行以下命令查看统计结果
# 例如 On node2
cd /home/modules/flink-1.8.0/log/
tail -200f flink-root-taskexecutor-0-node2.out
# TODO: 截图2.2, 终端窗口

# 安装ZooKeeper
# 使用复制命令生成配置文件
cd /home/modules/apache-zookeeper-3.5.7-bin/conf/
cp zoo_sample.cfg zoo.cfg
# 修改zoo.cfg配置文件，在zoo.cfg中添加以下内容。
vim zoo.cfg

## {zoo.cfg:找到 dataDir=/tmp/zookeeper 那行，用前两行替换，剩下四行直接添加到文件中}

dataDir=/home/modules/apache-zookeeper-3.5.7-bin/data
dataLogDir=/home/modules/apache-zookeeper-3.5.7-bin/logs
server.1=node1iip:2888:3888
server.2=node2iip:2888:3888
server.3=node3iip:2888:3888
server.4=node4iip:2888:3888

## end}

cd .. #进入上层目录
# 或 cd /home/modules/apache-zookeeper-3.5.7-bin/
mkdir -p data
mkdir -p logs
echo "1" > data/myid

# 将ZooKeeper安装文件夹复制到其他节点。
cd /home/modules/
scp -r apache-zookeeper-3.5.7-bin root@node2:$PWD
scp -r apache-zookeeper-3.5.7-bin root@node3:$PWD
scp -r apache-zookeeper-3.5.7-bin root@node4:$PWD

#登录 node2、node3、node4，修改 myid 内容。
# On node2
cd /home/modules/apache-zookeeper-3.5.7-bin
echo "2" > data/myid
# On node3
cd /home/modules/apache-zookeeper-3.5.7-bin
echo "3" > data/myid
# On node4
cd /home/modules/apache-zookeeper-3.5.7-bin
echo "4" > data/myid

# 验证ZooKeeper集群是否安装成功，在四个节点分别运行如下命令。leader 的选择与启动顺序有关
# 在四个节点都启动
zkServer.sh start
# 查看各个节点状态(全部节点启动后再进行查询，否则会失败)
zkServer.sh status
# TODO: 截图3.1, 4台服务器都运行, 含上面两条指令, 共4张
# 使用stop命令终止集群运行
zkServer.sh stop

# 修改配置文件

# 停止Flink的standalone模式，并启动ZooKeeper和Hadoop集群服务。
stop-cluster.sh
start-all.sh
zkServer.sh start
hadoop dfsadmin -report
zkServer.sh status

# On node1
# 修改FLink的配置文件。
cd /home/modules/flink-1.8.0/conf/
# 修改flink-conf.yaml配置文件。
vim flink-conf.yaml
# 修改masters配置文件。
vim masters

## {flink-conf.yaml: 直接添加到文件中
high-availability: zookeeper
high-availability.storageDir: hdfs://node1:8020/flink
high-availability.zookeeper.path.root: /flink
high-availability.zookeeper.quorum: node1:2181,node2:2181,node3:2181,node4:2181
## end}


## {masters: 覆盖原文件内容
node1:8081
node2:8081
## end}

# On node1
# 在HDFS上创建flink文件夹
hadoop fs -mkdir -p /flink

# 分发配置文件
cd /home/modules/flink-1.8.0/conf/
scp -r flink-conf.yaml masters root@node2:$PWD
scp -r flink-conf.yaml masters root@node3:$PWD
scp -r flink-conf.yaml masters root@node4:$PWD

# 之前的操作应该就作了，确认一下
cd /home/modules/flink-1.8.0/lib/
scp -r flink-shaded-hadoop-2-uber-2.7.5-10.0.jar root@node2:$PWD
scp -r flink-shaded-hadoop-2-uber-2.7.5-10.0.jar root@node3:$PWD
scp -r flink-shaded-hadoop-2-uber-2.7.5-10.0.jar root@node4:$PWD

# 启动Flink集群
# 正常启动Flink，jps可以看到Flink对应的进程已经正常启动。
start-cluster.sh
jps

访问node1服务器的Web界面，直接在浏览器中访问http://node1ip:8081/#/overview，node2服务器类似。

# 在HA环境下提交任务与standalone单节点模式下是一样的，即使JobManager所在服务器宕机也没有关系，JobManager会自动切换。

# 在node1上启动Socket服务，输入单词。
nc -lk 9000
# TODO: 截图3.2, 单词
# 另外打开一个node1的shell页面，在node1上启动Flink自带的单词统计程序，接收输入的Socket数据并进行统计。
cd /home/modules/flink-1.8.0
bin/flink run examples/streaming/SocketWindowWordCount.jar --hostname node1 --port 9000
# 执行以下命令并查看统计结果
cd /home/modules/flink-1.8.0/log/
tail -200f flink-root-taskexecutor-0-node1.out

# 模拟宕机情况实现自动切换

# 将node1服务器的JobManager进程关闭，过一段时间之后查看node2的JobManager是否能够访问。
jps
kill StandaloneSessionClusterEntrypoint_PID #改成实际pid
jps

关闭node1的JobMananger后刷新Web页面已经无法访问了。
http://node1ip:8081/#/overview
但是node2的Web界面依旧能够正常访问，并且能够看到jobmanager.rpc.address地址已经变为node2。
http://node2ip:8081/#/overview

# TODO: 截图3.3, Web管理界面的Job Manager目录

# 将上个实验启动的进程全部关闭
stop-cluster.sh
stop-all.sh
zkServer.sh stop

# 修改yarn-site.xml配置文件
# 在node1上添加以下配置属性到该文件中进行修改。
cd /home/modules/hadoop-2.7.7/etc/hadoop/
vim yarn-site.xml

## {yarn-site.xml: 添加到 `<configuration></configuration>` 标签内
<property>
  <name>yarn.resourcemanager.am.max-attempts</name>
  <value>4</value>
  <description>The maximum number of application master execution attempts.</description>
</property>
## end
# 然后将修改后的配置文件分发到其他节点上
cd /home/modules/hadoop-2.7.7/etc/hadoop/
scp -r yarn-site.xml root@node2:$PWD
scp -r yarn-site.xml root@node3:$PWD
scp -r yarn-site.xml root@node4:$PWD

# 修改Flink配置文件
# 在node1上执行以下命令修改配置文件
cd /home/modules/flink-1.8.0/conf/
vim flink-conf.yaml

## {flink-conf.yaml:用以下文本覆盖上一个小实验添加的配置
high-availability: zookeeper
high-availability.storageDir: hdfs://node1:8020/flink_yarn_ha
high-availability.zookeeper.path.root: /flink-yarn
high-availability.zookeeper.quorum: node1:2181,node2:2181,node3:2181,node4:2181
yarn.application-attempts: 10
## end}

# 启动HDFS和Yarn集群
start-all.sh

# 在4个节点上启动ZooKeeper
zkServer.sh start

# 在HDFS上创建文件夹
hadoop fs -mkdir -p /flink_yarn_ha
hadoop fs -ls /

# 在Yarn中启动Flink集群
# 直接在node1上执行以下命令，在Yarn中启动一个全新的Flink集群。可以使用--help查看yarn-session.sh的参数设置。
cd /home/modules/flink-1.8.0/
bin/yarn-session.sh -n 2 -jm 1024 -tm 1024 -d

# 查看Yarn管理界面
# 访问Yarn的 8088 管理界面<http://node1ip:8088/cluster>，可以看到其中有一个应用，这是为Flink单独启动的一个Session。
# TODO: 截图4.1, 含应用的yarn管理页面

# 提交任务
cd ~
# 在HDFS上创建文件夹并上传文件
hadoop fs -mkdir -p /flink_input
hadoop fs -put wordcount.txt /flink_input
# 在node1上执行以下命令，提交任务到Flink集群。
cd /home/modules/flink-1.8.0
bin/flink run examples/batch/WordCount.jar -input hdfs://node1:8020/flink_input -output hdfs://node1:8020/flink_output/wordcount-result.txt
# TODO: 截图4.2, 命令运行结果
# 如果不是第一次执行且出现报错，可以尝试运行下面这个指令
# hadoop fs -rm /flink_output/wordcount-result.txt
# 查看输出文件内容
hadoop fs -cat /flink_output/wordcount-result.txt
# TODO: 截图4.3, 命令运行结果

# 安装Kafka
cd /home/modules/
# 将Kafka安装包分发到各个节点
scp -r kafka_2.10-0.8.2.1 root@node2:$PWD
scp -r kafka_2.10-0.8.2.1 root@node3:$PWD
scp -r kafka_2.10-0.8.2.1 root@node4:$PWD

# 进入各个节点Kafka安装包的config目录，在server.properties配置文件中添加以下属性。
cd /home/modules/kafka_2.10-0.8.2.1/config
vim server.properties
## {server.properties: 每个节点用以下内容(每个节点3行)覆盖含 `broker.id=` 那一行
# node1
broker.id=1
host.name=node1
zookeeper.connect=node1:2181,node2:2181,node3:2181,node4:2181

# node2
broker.id=2
host.name=node2
zookeeper.connect=node1:2181,node2:2181,node3:2181,node4:2181

# node3
broker.id=3
host.name=node3
zookeeper.connect=node1:2181,node2:2181,node3:2181,node4:2181

# node4
broker.id=4
host.name=node4
zookeeper.connect=node1:2181,node2:2181,node3:2181,node4:2181
## end}

# 验证是否安装成功(前提要启动ZooKeeper)
zkServer.sh start
# 在4个节点分别启动Kafka。
cd /home/modules/kafka_2.10-0.8.2.1/config/
kafka-server-start.sh -daemon server.properties
jps
# TODO: 截图5.1, 4台服务器都运行jps, 共4张

# 停止所有服务并重启服务器
stop-cluster.sh
stop-all.sh
zkServer.sh stop
kafka-server-stop.sh

# 重启服务器，重启后记得改hosts

# 在node1节点启动Hadoop和Yarn集群。
start-all.sh
# start-cluster.sh
# On node1 启动yarn-session
cd /home/modules/flink-1.8.0/
bin/yarn-session.sh -n 2 -jm 1024 -tm 1024 -d
# 另外开启一个node1的终端，启动KafKa自带的ZooKeeper。
cd /home/modules/kafka_2.10-0.8.2.1
./bin/zookeeper-server-start.sh config/zookeeper.properties
# 另外开启一个node1的终端，启动node1的Kafka。
cd /home/modules/kafka_2.10-0.8.2.1/config/
kafka-server-start.sh -daemon server.properties
jps
# 创建一个自定义名称为 test 的 topic，在终端启动一个生产者。
cd /home/modules/kafka_2.10-0.8.2.1
./bin/kafka-topics.sh --create --zookeeper node1:2181 --replication-factor 1 --partitions 1 --topic wordsendertest
kafka-console-producer.sh --broker-list node1:9092 --topic test
# TODO: 截图5.3, 生产者终端, 截图含以上两条指令

# 另启动一个node1的终端，运行jar包。记录yarn-session应用所在服务器及端口(在运行结果最后一行左右)
cd ~
flink run -c kafkawc.WordCount WordCount.jar
# TODO: 截图5.3, 终端图

# 在生产者终端中输入单词
# TODO: 截图5.3，单词

在华为云安全组里开启之前记录的Yarn Session所在的端口
访问 <http://nodenip:port/#/taskmanagers>, 进入Task Managers目录下点击正在运行的任务。进入后点击Stdout页面查看读取到的Kafka数据。
TODO: nodenip为jar包输出结果中Yarn Session所在的机器公网ip;port为对应的端口
TODO: 截图5.3, Web界面输出结果

# 停掉所有服务,关机
flink list #查看所有flink任务,记录要停止的id
flink cancel jobID #jobID 为list里找到的任务id
yarn application -kill app-id # app-id 为yarn-session中应用的名称
cd /home/modules/kafka_2.10-0.8.2.1
./bin/zookeeper-server-stop.sh config/zookeeper.properties
cd /home/modules/kafka_2.10-0.8.2.1/config/
kafka-server-stop.sh -daemon server.properties
stop-all.sh
